---
title: 'Create Chat Completion'
api: 'POST /chat/completions'
description: 'Generate text responses'

---

This endpoint generates a response from the model based on a conversation history.

<RequestExample>
```bash cURL
curl -X POST https://api.blackbox.ai/chat/completions \
-H "Authorization: Bearer YOUR_API_KEY" \
-H "Content-Type: application/json" \
-d '{
    "model": "blackboxai/openai/gpt-4",
    "messages": [
        {
            "role": "user",
            "content": "What is the capital of France?"
        }
    ],
    "temperature": 0.7,
    "max_tokens": 256,
    "stream": false
}'
```

```javascript Node.js
const API_KEY = "YOUR_API_KEY";
const API_URL = "https://api.blackbox.ai/chat/completions";

const data = {
    "model": "blackboxai/openai/gpt-4",
    "messages": [
        {
            "role": "user",
            "content": "What is the capital of France?"
        }
    ],
    "temperature": 0.7,
    "max_tokens": 256,
    "stream": false
};

const response = await fetch(API_URL, {
    method: 'POST',
    headers: {
        'Authorization': `Bearer ${API_KEY}`,
        'Content-Type': 'application/json'
    },
    body: JSON.stringify(data)
});

const responseData = await response.json();
console.log(responseData);
```

```python Python
import requests

API_KEY = "YOUR_API_KEY"
API_URL = "https://api.blackbox.ai/chat/completions"

headers = {
  "Authorization": f"Bearer {API_KEY}",
  "Content-Type": "application/json"
}

data = {
    "model": "blackboxai/openai/gpt-4",
    "messages": [
        {
            "role": "user",
            "content": "What is the capital of France?"
        }
    ],
    "temperature": 0.7,
    "max_tokens": 256,
    "stream": False
}

response = requests.post(API_URL, headers=headers, json=data)
print(response.json())
```

</RequestExample>

### Request Body

<ParamField body="model" type="string" required>
  The ID of the model to use. See the [Models page](/api-reference/models/chat-models) for available options.
</ParamField>
<ParamField body="messages" type="array" required>
  An array of message objects representing the conversation history. Each object must have a `role` (`user` or `assistant`) and `content` (the message text).
</ParamField>
<ParamField body="stream" type="boolean">
  Defaults to `false`. If set to `true`, the response will be streamed back in chunks as it's generated.
</ParamField>
<ParamField body="max_tokens" type="integer">
  The maximum number of tokens to generate in the completion. Defaults to `1024`.
</ParamField>
<ParamField body="temperature" type="number">
  Controls randomness. A lower value (e.g., `0.2`) makes the output more deterministic, while a higher value (e.g., `0.8`) makes it more random. Defaults to `1`.
</ParamField>
<ParamField body="top_p" type="number">
  An alternative to sampling with temperature, called nucleus sampling. Defaults to `1`.
</ParamField>
<ParamField body="agent" type="string">
  An optional parameter to specify a specialized agent for the task.
</ParamField>
<ParamField body="response_format" type="object">
  An object specifying the format that the model must output. Setting `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.
</ParamField>


<ResponseExample>
```json Response
{
  "id":"gen-...",
  "created":1757140020,
  "model":"openai/gpt-4",
  "object":"chat.completion",
  "system_fingerprint":"None",
  "choices":[
    {
      "finish_reason":"stop",
      "index":0,
      "message":{
        "content":"The capital of France is Paris.",
        "role":"assistant",
        "tool_calls":"None",
        "function_call":"None"
      },
      "provider_specific_fields":{
        "native_finish_reason":"stop"
      }
    }
  ],
  "usage":{
    "completion_tokens":7,
    "prompt_tokens":14,
    "total_tokens":21,
    "completion_tokens_details":{
      "accepted_prediction_tokens":"None",
      "audio_tokens":"None",
      "reasoning_tokens":0,
      "rejected_prediction_tokens":"None"
    },
    "prompt_tokens_details":{
      "audio_tokens":0,
      "cached_tokens":0
    }
  },
  "provider":"OpenAI"
}
```
</ResponseExample>

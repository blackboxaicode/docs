---
title: "Chat"
description: "API Reference for the /v1/chat endpoint."
---

This endpoint generates a response from the model based on a conversation history.

<RequestExample>
```bash cURL
curl -X POST [https://www.blackbox.ai/api/v1/chat](https://www.blackbox.ai/api/v1/chat) \
-H "Content-Type: application/json" \
-H "Authorization: Bearer YOUR_API_KEY" \
-d '{
  "model": "blackbox-chat",
  "messages": [
    {
      "role": "user",
      "content": "Write a short story about an astronaut."
    }
  ],
  "temperature": 0.7,
  "max_tokens": 256,
  "stream": false
}'
```

```javascript Node.js
const fetch = require("node-fetch");

const API_KEY = "YOUR_API_KEY";

async function generateChat() {
    const response = await fetch(
        "[https://www.blackbox.ai/api/v1/chat](https://www.blackbox.ai/api/v1/chat)",
        {
            method: "POST",
            headers: {
                "Content-Type": "application/json",
                Authorization: `Bearer ${API_KEY}`,
            },
            body: JSON.stringify({
                model: "blackbox-chat",
                messages: [
                    {
                        role: "user",
                        content: "Write a short story about an astronaut.",
                    },
                ],
                temperature: 0.7,
                max_tokens: 256,
                stream: false,
            }),
        }
    );

    const data = await response.json();
    console.log(data.choices[0].message.content);
}

generateChat();
```

```python Python
import requests

API_KEY = "YOUR_API_KEY"
API_URL = "[https://www.blackbox.ai/api/v1/chat](https://www.blackbox.ai/api/v1/chat)"

headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {API_KEY}"
}

data = {
    "model": "blackbox-chat",
    "messages": [
        {"role": "user", "content": "Write a short story about an astronaut."}
    ],
    "temperature": 0.7,
    "max_tokens": 256,
    "stream": False
}

response = requests.post(API_URL, headers=headers, json=data)
print(response.json()['choices'][0]['message']['content'])
```

</RequestExample>

### Request Body

<ParamField body="model" type="string" required>
  The ID of the model to use. See the [Models page](/api-reference/models/chat-models) for available options.
</ParamField>
<ParamField body="messages" type="array" required>
  An array of message objects representing the conversation history. Each object must have a `role` (`user` or `assistant`) and `content` (the message text).
</ParamField>
<ParamField body="stream" type="boolean">
  Defaults to `false`. If set to `true`, the response will be streamed back in chunks as it's generated.
</ParamField>
<ParamField body="max_tokens" type="integer">
  The maximum number of tokens to generate in the completion. Defaults to `1024`.
</ParamField>
<ParamField body="temperature" type="number">
  Controls randomness. A lower value (e.g., `0.2`) makes the output more deterministic, while a higher value (e.g., `0.8`) makes it more random. Defaults to `1`.
</ParamField>
<ParamField body="top_p" type="number">
  An alternative to sampling with temperature, called nucleus sampling. Defaults to `1`.
</ParamField>
<ParamField body="agent" type="string">
  An optional parameter to specify a specialized agent for the task.
</ParamField>
<ParamField body="response_format" type="object">
  An object specifying the format that the model must output. Setting `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.
</ParamField>

### Response Body

<ResponseExample>
```json Response
{
  "id": "chatcmpl-...",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "blackbox-chat",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "The lone astronaut, Commander Eva, stared out at the swirling nebula..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 12,
    "completion_tokens": 50,
    "total_tokens": 62
  }
}
```
</ResponseExample>
